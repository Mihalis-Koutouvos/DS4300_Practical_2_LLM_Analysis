{
    "processed_pdfs": [
        {
            "title": "01 - Introduction & Getting Started",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials",
                    "released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being",
                    "3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication",
                    "released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations",
                    "a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ]
            }
        },
        {
            "title": "02 - Foundations",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8",
                    "output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value",
                    "front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst",
                    "output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14",
                    "tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ]
            }
        },
        {
            "title": "03 - Moving Beyond the Relational Model",
            "chunked_content": {
                "200_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "200_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102",
                    "changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23",
                    "up something consistency availability or tolerance to failure 22 23"
                ],
                "200_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has",
                    "same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance",
                    "changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13",
                    "end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ]
            }
        },
        {
            "title": "04 - Data Replication",
            "chunked_content": {
                "200_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "200_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and",
                    "how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "200_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13",
                    "the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the",
                    "how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes",
                    "to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ]
            }
        },
        {
            "title": "04-B+Tree Walkthrough",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2",
                    "state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this",
                    "descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root",
                    "state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ]
            }
        },
        {
            "title": "05 - NoSQL Intro + KV DBs",
            "chunked_content": {
                "200_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "200_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the",
                    "the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of",
                    "redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "200_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of",
                    "notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if",
                    "the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are",
                    "be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments",
                    "bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored",
                    "insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ]
            }
        },
        {
            "title": "05b - Redis in Docker",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "200_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed",
                    "connection if they arent already installed"
                ],
                "200_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed",
                    "use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ]
            }
        },
        {
            "title": "06 - Redis + Python",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "200_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14",
                    "multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "200_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid",
                    "getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore",
                    "multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ]
            }
        },
        {
            "title": "07 - Document DBs and Mongo",
            "chunked_content": {
                "200_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "200_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is",
                    "databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "200_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document",
                    "be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb",
                    "databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password",
                    "replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ]
            }
        },
        {
            "title": "08 - PyMongo",
            "chunked_content": {
                "200_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "200_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "200_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7",
                    "venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ]
            }
        },
        {
            "title": "09 - Introduction to Graph Data Model",
            "chunked_content": {
                "200_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "200_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can",
                    "vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "200_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted",
                    "types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes",
                    "vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22",
                    "of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ]
            }
        },
        {
            "title": "10 - Neo4j",
            "chunked_content": {
                "200_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "200_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create",
                    "browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "200_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j",
                    "image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import",
                    "browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22",
                    "create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ]
            }
        },
        {
            "title": "C12-bst",
            "chunked_content": {
                "200_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "200_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6",
                    "postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9",
                    "successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "200_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder",
                    "the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in",
                    "postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6",
                    "all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else",
                    "13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the",
                    "we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26",
                    "x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ]
            }
        },
        {
            "title": "DS4300 - Lecture Notes",
            "chunked_content": {
                "200_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "200_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on",
                    "indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "200_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am",
                    "block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of",
                    "indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion",
                    "time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ]
            }
        },
        {
            "title": "ICS 46 Spring 2022, Notes and Examples- AVL Trees",
            "chunked_content": {
                "200_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet",
                    "that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up",
                    "to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "200_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too",
                    "anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be",
                    "h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least",
                    "not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions",
                    "empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2",
                    "8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "200_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree",
                    "very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the",
                    "anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to",
                    "start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and",
                    "except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness",
                    "we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good",
                    "this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet",
                    "not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search",
                    "work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique",
                    "runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up",
                    "empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free",
                    "despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result",
                    "n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes",
                    "a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ]
            }
        },
        {
            "title": "Lecture 2 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst",
                    "worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison",
                    "slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on",
                    "worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent",
                    "linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ]
            }
        },
        {
            "title": "Lecture 3 Notes (B-Trees) - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "structures and algorithms chapter 11"
                ],
                "200_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of",
                    "side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "200_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either",
                    "elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers",
                    "side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the",
                    "a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11",
                    "structures and algorithms chapter 11"
                ],
                "500_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "500_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "500_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11",
                    "structures and algorithms chapter 11"
                ],
                "1000_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "1000_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "1000_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ]
            }
        },
        {
            "title": "Lecture 4 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "200_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default",
                    "changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "200_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has",
                    "no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check",
                    "changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are",
                    "schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ]
            }
        },
        {
            "title": "Lecture 5 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "200_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from",
                    "maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry",
                    "1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "200_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can",
                    "pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but",
                    "maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or",
                    "very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model",
                    "is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300",
                    "friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ]
            }
        },
        {
            "title": "Lecture 6 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "200_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "200_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful",
                    "4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ]
            }
        },
        {
            "title": "Lecture 7 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "200_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4",
                    "i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "200_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for",
                    "commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4",
                    "i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ]
            }
        }
    ]
}