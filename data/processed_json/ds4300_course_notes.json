{
    "processed_pdfs": [
        {
            "title": "01 - Introduction & Getting Started",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials",
                    "released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get",
                    "slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being",
                    "3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication",
                    "released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations",
                    "a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17",
                    "conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want",
                    "distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval mark fontenot phd northeastern university hi mark fontenot phd ofﬁce 353 meserve hall ofﬁce hours m th 130 300 pm if those times dont work just dm me on slack to set up an alternate time usually very available on slack so just dm me mfontenotnortheasternedu 2 teaching assistants 3 iker acosta venegas dallon archibald nathan cheung aryan jain abhishek kumar eddy liu sevinch noori junxiang lin where do i ﬁnd course materials notes assignments etc httpsmarkfontenotnetteachingds430025sds4300 assignment submissions and grades gradescope q a platform is campuswire quick dms and announcements will be on slack 4 whats this class about by the end of this class you should understand the efﬁciencyrelated concepts including limitations of rdbmss understand data replication and distribution effects on typical db usage scenarios understand the use cases for and data models of various nosql database systems including storing and retrieving data data models include documentbased keyvalue stores graph based among others access and implement data engineering and bigdatarelated aws services 5 course deliverables and evaluation 6 assignments homeworks and practicals usually due tuesday nights at 1159 unless otherwise stated 3 bonus for submitting 48 hours early no you cant get 3 for submitting 48 hours early no late submissions accepted but life happens so everyone gets 1 free noquestionsasked 48 hour extension dm dr fontenot on slack sometime before the original deadline requesting to use your extension 7 assignments submissions will be via gradescope andor github unless directed otherwise only submit pdfs unless otherwise instructed if only submitting a pdf be sure to associate questions in gradescope with the correct page in your pdf failure to do so may result in a grade of 0 on the assignment all regrade requests must be submitted within 48 hours of grades being released on gradescope no exceptions 8 midterm monday march 17 mark it in your calendars now 9 final grade breakdown homeworks 5 30 practicals 2 20 midterm 20 semester project 30 10 reference materials primary resources 11 oreilly playlist other books are in the playlist i will add additional materials to the playlist or webpage as the semester progresses tentative list of topics thinking about data storage and retrieval at the data structures level how far can we get with the relational model nosql databases document databases mongo graph databases neo4j keyvalue databases maybe vector databases data distribution and replication distributed sql dbs apache sparksparksql big data tools and services on aws 12 tools you will need to install on your laptop docker desktop anaconda or miniconda python youre welcome to use another distro but youre responsible for ﬁxing it if something doesnt work dependency conﬂicts etc a database access tool like datagrip or dbeaver vs code set up for python development see here for more info about vscode python and anaconda ability to interact with git and github through terminal or gui app 13 topics to review over the next few days shellcmd promptpowershell cli windows if you want a unix terminal wsl2 or zsh on windows navigating the ﬁle system running commands like pip conda python etc command line args docker docker compose basics of dockerﬁles and dockercomposeyaml ﬁles port mapping setting up volumes mapping between host and guest os 14 is your python rusty or havent done a ton with it python crash course by net ninja on yt on oreilly see python section of class playlist python objectoriented programming video course by simon sez it e matthes python crash course 3rd edition no starch press not related to the yt video playlist listed above 15 expectations conduct yourself respectfully dont distract your classmates from learning dont cheat do your own work unless group assignment discussing problems is encouraged but you must formulate your own solutions see syllabus for details 16 lets gooo 17"
                ]
            }
        },
        {
            "title": "02 - Foundations",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8",
                    "output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array",
                    "table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value",
                    "front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst",
                    "output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search",
                    "do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14",
                    "tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on",
                    "case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efﬁciency is linear search start at the beginning of a list and proceed element by element until you ﬁnd what youre looking for you get to the last element and havent found it 2 searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute 3 lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 5 6 records contiguously allocated array front back 6 records linked by memory addresses linked list extra storage for a memory address pros and cons arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access 6 insert after 2nd record records records 5 records had to be moved to make space insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 8 a c g m p r z target a mid since target arrmid we reset right to mid 1 left right a c g m p r z target a mid left right time complexity linear search best case target is found at the ﬁrst element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity 9 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 10 back to database searching assume data is stored on disk by column ids value searching for a speciﬁc id fast but what if we want to search for a speciﬁc specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefﬁcient 11 we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and ﬁnd its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 13 image from httpscoursesgraingerillinoiseducs225sp2019notesbst to the board 14"
                ]
            }
        },
        {
            "title": "03 - Moving Beyond the Relational Model",
            "chunked_content": {
                "200_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "200_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102",
                    "changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23",
                    "up something consistency availability or tolerance to failure 22 23"
                ],
                "200_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the",
                    "one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has",
                    "same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance",
                    "changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13",
                    "end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently",
                    "scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states",
                    "no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "500_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result",
                    "amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the",
                    "that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_0": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_50": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ],
                "1000_words_overlap_100": [
                    "moving beyond the relational model mark fontenot phd northeastern university beneﬁts of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efﬁciency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simpliﬁed error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads 6 isolation dirty read 7 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained dirty read a transaction t1 is able to read a row that has been modiﬁed by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read 8 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads 9 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer 10 delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id continued next slide example transaction transfer 11 continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and ﬁnancial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions 16 single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem 19 the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view 20 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not",
                    "database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem database view 21 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23"
                ]
            }
        },
        {
            "title": "04 - Data Replication",
            "chunked_content": {
                "200_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "200_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and",
                    "how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "200_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of",
                    "the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13",
                    "the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the",
                    "how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes",
                    "to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency",
                    "to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading",
                    "method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "500_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients",
                    "storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25",
                    "newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_0": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_50": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ],
                "1000_words_overlap_100": [
                    "replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data beneﬁts 2 scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes but ok for data warehouse applications high read volumes 5 aws ec2 pricing oct 2024 6 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware 7 data replication vs partitioning 8 replicates have same data as main partitions have a subset of the data replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication 12 this write could not be sent to one of the followers only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers 14 replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difﬁculty in handling concurrent transactions writeahead log wal a bytelevel speciﬁc log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difﬁcult logical rowbased log for relational dbs inserted rows modiﬁed rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger ﬁres in response to an insert update or delete flexible because you can have application speciﬁc replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesnt wait for conﬁrmation 15 synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy perhaps based on who has the most updates use a controller node to appoint new leader and how do we conﬁgure clients to start writing to the new leader 16 what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conﬂicting data split brain no way to resolve conﬂicting requests leader failure detection optimal timeout is tricky 17 replication lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window replication lag 18 readafterwrite consistency scenario youre adding a comment to a reddit post after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modiﬁable data from the clients perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for recently updated data for example have a policy that all requests within one minute of last update come from leader 21 but this can create its own challenges 22 we created followers so they would be proximal to users but now we have to route requests to distant leaders when reading modiﬁable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent preﬁx reads reading data out of order can occur if different partitions replicate data at different rates there is no global write consistency consistent preﬁx read guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 a b how far into the future can you see ms b about 10 seconds usually mr a 25"
                ]
            }
        },
        {
            "title": "04-B+Tree Walkthrough",
            "chunked_content": {
                "200_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "200_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2",
                    "state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "200_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we",
                    "node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this",
                    "descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root",
                    "state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "500_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_0": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_50": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ],
                "1000_words_overlap_100": [
                    "large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university 2 b tree m 4 insert 42 21 63 89 initially the ﬁrst node is a leaf node and root node 21 42 represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split 3 b tree m 4 insert 35 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 52 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 4 b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to ﬁnd out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split 5 b tree m 4 insert 30 starting at root we descend to the leftmost child well call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes 6 b tree m 4 insert 30 contd redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do 7 b tree m 4 fast forward to this state of the tree observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 8 b tree m 4 insert 37 step 1 9 b tree m 4 insert 37 step 2 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root"
                ]
            }
        },
        {
            "title": "05 - NoSQL Intro + KV DBs",
            "chunked_content": {
                "200_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "200_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the",
                    "the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of",
                    "redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "200_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that",
                    "deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of",
                    "notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if",
                    "the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are",
                    "be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda",
                    "designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across",
                    "store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments",
                    "bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored",
                    "insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored",
                    "in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to",
                    "by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "500_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could",
                    "system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed",
                    "browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price",
                    "or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38",
                    "sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_0": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_50": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ],
                "1000_words_overlap_100": [
                    "nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conﬂicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can 2 see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conﬂicts are unlikely to occur even if there is a conﬂict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modiﬁed 3 optimistic concurrency low conﬂict systems backups analytical dbs etc read heavy systems the conﬂicts that arise can be handled by rolling back and rerunning a transaction that notices a conﬂict so optimistic concurrency works well allows for higher concurrency high conﬂict systems rolling back and rerunning transactions that encounter a conﬂict less efﬁcient so a locking scheme pessimistic model might be preferable 4 nosql nosql ﬁrst used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data 5 httpswwwdataversitynetabriefhistoryofnonrelationaldatabases cap theorem review 6 reference httpsalperenbayramoglucompostsunderstandingcaptheorem you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid cap theorem review 7 reference httpsalperenbayramoglucompostsunderstandingcaptheorem consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time 8 acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent 9 acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user proﬁles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at speciﬁc intervals or b appendonly ﬁle which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis",
                    "in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments conﬁg settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type 27 value of kv entry is a collection of ﬁeldvalue pairs use cases can be used to represent basic objectsstructures number of ﬁeldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands 28 hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 what is returned list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 10 front back nil list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 lpush mylist one lpush mylist two lpush mylist three json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 36 set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300 37 38"
                ]
            }
        },
        {
            "title": "05b - Redis in Docker",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "200_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed",
                    "connection if they arent already installed"
                ],
                "200_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed",
                    "use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "500_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_0": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_50": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ],
                "1000_words_overlap_100": [
                    "redis in docker setup mark fontenot phd northeastern university prerequisites 2 you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to ﬁnd the redis image click run 3 step 2 conﬁgure run the container give the new container a name enter 6379 in host port ﬁeld click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 conﬁgure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful 6 there will be a message to install drivers above test connection if they arent already installed"
                ]
            }
        },
        {
            "title": "06 - Redis + Python",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "200_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14",
                    "multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "200_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget",
                    "list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid",
                    "getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore",
                    "multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "500_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_0": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_50": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ],
                "1000_words_overlap_100": [
                    "redis python mark fontenot phd northeastern university redispy 2 redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings 3 import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val 5 string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server less network overhead 12 r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4 redis in context 13 redis in ml simpliﬁed example 14 source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml 15 source httpsmadewithmlcomcoursesmlopsfeaturestore"
                ]
            }
        },
        {
            "title": "07 - Document DBs and Mongo",
            "chunked_content": {
                "200_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "200_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is",
                    "databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "200_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must",
                    "keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document",
                    "be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb",
                    "databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password",
                    "replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select",
                    "for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were",
                    "from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "500_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds",
                    "atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and",
                    "released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_0": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_50": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ],
                "1000_words_overlap_100": [
                    "document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple ﬂexible and scalable 2 what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format 3 json syntax 4 httpswwwjsonorgjsonenhtml binary json bson bson binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efﬁcient encoding and decoding must be efﬁcient supported by many modern programming languages 5 xml extensible markup language precursor to json as data exchange format xml css web pages that separated content and formatting structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving speciﬁc elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type deﬁnition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 documentdb as a service 10 httpswwwmongodbcomcompanyourstory mongodb structure 11 database collection a collection b collection c document 1 document 2 document 3 document 1 document 2 document 3 document 1 document 2 document 3 mongodb documents no predeﬁned schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db 13 rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document ﬁelds replication supports replica sets with automatic failover load balancing built in 14 mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged 15 interacting with mongodb mongosh mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode 16 mongodb community edition in docker create a container map hostcontainer port 27017 give initial username and password for superuser 17 e d mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mﬂix download mﬂix sample dataset and unzip it import json ﬁles for users theaters movies and comments into new collections in the mﬂix database 19 creating a database and collection 20 mﬂix users to create a new db to create a new collection mongosh mongo shell ﬁnd is like select 21 collectionfind ____ ____ ﬁlters projections mongosh ﬁnd select from users 22 use mflix dbusersfind mongosh ﬁnd select from users where name davos seaworth 23 dbusersfindname davos seaworth ﬁlter mongosh ﬁnd select from movies where rated in pg pg13 24 dbmoviesfindrated in pg pg13 mongosh ﬁnd return movies which were released in mexico and have an imdb rating of at least 7 25 dbmoviesfind countries mexico imdbrating gte 7 mongosh ﬁnd return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama 26 dbmoviesfind year 2010 or awardswins gte 5 genres drama comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama 28 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama 29 dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 1 return 0 dont return pymongo 30 pymongo pymongo is a python library for interfacing with mongodb instances 31 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35",
                    "collection 32 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 collection dbmycollection inserting a single document 33 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in collection select count from collection 34 demodbcollectioncount_documents 35"
                ]
            }
        },
        {
            "title": "08 - PyMongo",
            "chunked_content": {
                "200_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "200_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "200_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7",
                    "venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "500_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_0": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_50": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ],
                "1000_words_overlap_100": [
                    "mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances 2 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 getting a database and collection 3 from pymongo import mongoclient client mongoclient mongodbuser_namepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document 4 db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id find all movies from 2000 5 from bsonjson_util import dumps find all movies released in 2000 movies_2000 dbmoviesfindyear 2000 print results printdumpsmovies_2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip ﬁle contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the ﬁles and run jupyter lab 6 7"
                ]
            }
        },
        {
            "title": "09 - Introduction to Graph Data Model",
            "chunked_content": {
                "200_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "200_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can",
                    "vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "200_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship",
                    "it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted",
                    "types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes",
                    "vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22",
                    "of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "500_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class",
                    "ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_0": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_50": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ],
                "1000_words_overlap_100": [
                    "introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identiﬁed each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes things like instagram but also modeling social interactions in ﬁelds like psychology and sociology the web it is just a big graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns lives_with married_to properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 7 1 2 3 6 5 4 ex 1 2 6 5 not a path 1 2 6 2 3 flavors of graphs connected vs disconnected there is a path between any two nodes in the graph weighted vs unweighted edge has a weight property important for some algorithms directed vs undirected relationships edges deﬁne a start and end node acyclic vs cyclic graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathﬁnding pathﬁnding ﬁnding the shortest path between two nodes if one exists is probably the most common operation shortest means fewest edges or lowest weight average shortest path can be used to monitor efﬁciency and resiliency of networks minimum spanning tree cycle detection maxmin ﬂow are other types of pathﬁnding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are more important in a network compared to other nodes ex social network inﬂuencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22"
                ]
            }
        },
        {
            "title": "10 - Neo4j",
            "chunked_content": {
                "200_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "200_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create",
                    "browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "200_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j",
                    "awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j",
                    "image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import",
                    "browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22",
                    "create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "500_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name",
                    "folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_0": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_50": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ],
                "1000_words_overlap_100": [
                    "neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler oreilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4js graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnect_toothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efﬁcient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose 5 supports multicontainer management setup is declarative using yaml dockercomposeyaml ﬁle services volumes networks etc 1 command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more well it works on my machine interaction is mostly via command line dockercomposeyaml 6 services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_apoc_export_file_enabledtrue neo4j_apoc_import_file_enabledtrue neo4j_apoc_import_file_use__neo4j__configtrue neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dblogslogs neo4j_dbimportvarlibneo4jimport neo4j_dbpluginsplugins never put secrets in a docker compose ﬁle use env ﬁles env files env ﬁles stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal envdev envprod 7 neo4j_passwordabc123 env file docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache 8 localhost7474 9 neo4j browser 10 httpsneo4jcomdocsbrowsermanualcurrentvisualtour localhost7474 then login inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser namealice match bobuser name bob create aliceknows since 20221201bob 12 note relationships are directed in neo4j matching which users were born in london match usruser birthplace london return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netﬂixzip ﬁle copy netﬂix_titlescsv into the following folder where you put your docker compose ﬁle neo4j_dbneo4j_dbimport 14 importing data 15 basic data importing load csv with headers from filenetflix_titlescsv as line createmovie id lineshow_id title linetitle releaseyear linerelease_year 16 type the following into the cypher editor in neo4j browser loading csvs general syntax load csv with headers from filefile_in_import_foldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name 19 adding edges load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm 20 gut check lets check the movie titled ray match mmovie title raydirectedpperson return m p 21 22"
                ]
            }
        },
        {
            "title": "C12-bst",
            "chunked_content": {
                "200_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "200_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6",
                    "postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9",
                    "successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "200_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is",
                    "the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder",
                    "the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in",
                    "postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6",
                    "all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return",
                    "return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor",
                    "empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else",
                    "13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the",
                    "we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26",
                    "x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the",
                    "keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question",
                    "predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "500_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property",
                    "nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8",
                    "and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child",
                    "x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this",
                    "n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_0": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_50": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ],
                "1000_words_overlap_100": [
                    "chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 2 traversal of the nodes in a bst by traversal we mean visiting all the nodes in a graph traversal strategies can be speciﬁed by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst ﬁnds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input well take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x 2 while y nil do 3 if keyy k then return y 4 else if keyy k then y righty 5 else y lefty 6 return not found 8 an example search for 8 7 4 2 6 9 13 11 nil what is the running time of search 9 2 the maximum and the minimum to ﬁnd the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to ﬁnd the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return empty tree 2 y x 3 while lefty nil do y lefty 4 return keyy bstmaximumx 1 if x nil then return empty tree 2 y x 3 while righty nil do y righty 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we ﬁnd a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return error 2 y x 3 while true do 4 if keyy k 5 then z lefty 6 else z righty 7 if z nil break 8 9 if keyy k then lefty z 10 else rightpy z 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for ﬁnding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete",
                    "11 15 19 20 7 25 23 14 algorithm bstsuccessorx 1 if rightx nil then 2 y rightx 3 while lefty nil do y lefty 4 return y 5 else 6 y x 7 while rightpx x do y px 8 if px nil then return px 9 else return no successor 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undeﬁned what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to zs place 3 if z has two children then we will identify zs successor call it y the successor y either is a leaf or has only the right child promote y to zs place treat the loss of y using one of the above two solutions 17 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z 3 else y bstsuccessorz 4 y is the node thats actually removed 5 here y does not have two children 6 if lefty nil 7 then x lefty 8 else x righty 9 x is the node thats moving to ys position 10 if x nil then px py 11 px is reset if x isnt nil 12 resetting is unnecessary if x is nil 19 algorithm contd 13 if py nil then roott x 14 if y is the root then x becomes the root 15 otherwise do the following 16 else if y leftpy 17 then leftpy x 18 if y is the left child of its parent then 19 set the parents left child to x 20 else rightpy x 21 if y is the right child of its parent then 22 set the parents right child to x 23 if y z then 24 keyz keyy 25 move other data from y to z 27 return y 20 summary of eﬃciency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys xn height of the tree of n keys yn 2xn we want an upper bound on eyn for n 2 we have eyn 1 n n x i1 2emaxyi1 yni emaxyi1 yni eyi1 yni eyi1 eyni collecting terms eyn 4 n n1 x i1 eyi 24 analysis we claim that for all n 1 eyn 1 4 n3 3 we prove this by induction on n base case ey1 20 1 induction step we have eyn 4 n n1 x i1 eyi using the fact that n1 x i0 i 3 3 n 3 4 eyn 4 n 1 4 n 3 4 eyn 1 4 n 3 3 25 jensens inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 fλx 1 λy λfx 1 λfy jensens inequality states that for all random variables x and for all convex function f fex efx let this x be xn and fx 2x then efx eyn so we have 2exn 1 4 n 3 3 the righthand side is at most n 33 by taking the log of both sides we have exn olog n thus the average height of a randomly build bst is olog n 26"
                ]
            }
        },
        {
            "title": "DS4300 - Lecture Notes",
            "chunked_content": {
                "200_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "200_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on",
                    "indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "200_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte",
                    "x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am",
                    "block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of",
                    "indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion",
                    "time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "500_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same",
                    "experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_0": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_50": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ],
                "1000_words_overlap_100": [
                    "foundations lecture 2 searching common operation performed by database system ex select linear search baseline for efficiency start sequentially record collection of values for attributes of a single entity instance a row of a table collection set of records of the same entity type a table search key a value for an attribute from the entity type 1 contiguous allocated list all nx bytes allocated as a single chunk of memory in the form of an array continuously in order faster for random access slow for random insertions slow for inserting anywhere but the end linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a chain using memory addresses may not be in order faster for random insertions binary search input array of values in sorted order target value output index of target value lecture 4 11525 cpu root processor registar memory that they use is expensive l1 cache l2 cache further away from processor ram sddhdd lots of storage persistent hard drive reading a block of byte 64 bit integer is 8 bytes if i wanted to reach it it can be read by the 2048 byte block size other abstracted layers data base management systems avl tree k v 64 bit int 4x8 bytes 32 bytes to index on disk storage vs in ram storage possible that every node is on a separate block on a hard drive 7 x 2048 byte block size but not optimizing the structure make better use of the 2048 bytes of memory in order to increase performance decrease height of avl tree 7 x32 bytes of memory to store 7 nodes maximize value of each disk block b tree nodes will have up to 128 256 values that i am indexing so just the keys indexing the data structure to minimize height of the tree cant keep them unsorted array because hard to know where the value would fall into between the values of the node k1 through k5 are on one block and each of the children are on a diff block lecture 5 1162025 set up group project github repo git classroom lecture 12725 relational models benefits standardized data model and query language acid compliance atomicity consistency isolation durability works well with highly structured data can handle large amounts of data well understood lots of tooling lots of experience increase efficiency indexing direct controlling of storage column oriented vs row oriented query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of 1 cfud operations performed as a single logical unit of work commit entire sequence succeeds rollback or abort entire seq fails ensures data integrity error recoverysimple error handling concurrency control reliable data handling atomicity transaction is an atomic unit of work all or nothing consistency transaction move from one consistent state to another consistent state consistent dsta data models are enforced isolation two transactions being executed at the same time but dont affect eh other or else leads to dirty read nonrepetable read phantom reads lecture 25 redis data types keys strings or any binary sequence values strings list sets hashes supports concurrency occurring in parallel two processing two datasets and indexing into redis introspection collects all the metadata for code completion"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -1-3",
            "chunked_content": {
                "200_words_overlap_0": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to",
                    "directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2",
                    "over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array",
                    "n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a",
                    "specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_50": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to",
                    "type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so",
                    "addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox",
                    "is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base",
                    "n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a",
                    "we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_100": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to",
                    "database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory",
                    "directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2",
                    "addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox",
                    "over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array",
                    "5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by",
                    "n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a",
                    "both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent",
                    "specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_0": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox",
                    "5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_50": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox",
                    "is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_100": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox",
                    "over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent",
                    "specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_0": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_50": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_100": [
                    "01 introduction getting started discusses the syllabus and requirements to succeed in the course 02 foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile and complex operation used to retrieve specific data based on conditions baseline search algorithm linear search the simplest and most basic search technique starts at the beginning of a list and proceeds element by element until you find what youre looking for you reach the last element and havent found it time complexity on where n is the number of records fundamental database concepts record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type used to locate records can consist of one or more attributes eg composite keys optimized search techniques indexing improves search performance by creating a sorted data structure eg btrees hash indexes binary search efficient for sorted data reducing search complexity to olog n hashing uses a hash function to directly map keys to storage locations achieving near o1 search time fulltext search used for searching textual data efficiently often with inverted indexes range queries searching for values within a certain range eg between in sql pattern matching searching with wildcards eg like in sql for partial string matches lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously touching allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses pros and cons arrays are faster for random access but slow for inserting anywhere but the end imagine you have a long row of mailboxes each with a number on it 1 2 3 and so on these mailboxes represent an array if you want to quickly find mailbox 5 you can just go straight to it thats fast random access finding things quickly but if you want to add a new mailbox between 2 and 3 youd have to shift all the mailboxes after 2 over by one spot thats slow insertion hard to add things in the middle linked lists are faster for inserting anywhere in the list but slower for random access now think of a linked list like a treasure hunt where each clue a mailbox tells you where the next one is if you want to insert a new mailbox between 2 and 3 you just change the clue at 2 to point to the new mailbox and the new mailbox points to 3 thats fast insertion easy to add things in the middle but if you want to find mailbox 5 you have to start at 1 and follow the clues one by one until you get there thats slow random access hard to find things quickly arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where the target is located or some value indicated the target was not found time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log base 2 n comparisons therefore in the worst case binary search is olog base 2 n time complexity back to database searching assume data is stored on disk by column ids value saerching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -13-15",
            "chunked_content": {
                "200_words_overlap_0": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly",
                    "linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many",
                    "keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b",
                    "tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf",
                    "nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "200_words_overlap_50": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly",
                    "key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the",
                    "random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace",
                    "key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf",
                    "tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf",
                    "consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "200_words_overlap_100": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly",
                    "2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for",
                    "linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many",
                    "random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace",
                    "keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b",
                    "it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1",
                    "tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf",
                    "locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers",
                    "nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "500_words_overlap_0": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace",
                    "it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "500_words_overlap_50": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace",
                    "key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "500_words_overlap_100": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace",
                    "keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers",
                    "nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "1000_words_overlap_0": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "1000_words_overlap_50": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ],
                "1000_words_overlap_100": [
                    "b trees btrees and b trees a btree is a selfbalancing search tree designed to keep keys sorted and allow searches sequential access insertions and deletions in logarithmic time it is optimized for systems that read and write large blocks of data like databases and file systems a b tree is an extension of a btree where internal nodes contain only keys and leaf nodes store the actual records or pointers to records this structure improves range queries and sequential access btree properties 1 each node can have at most m children where m is the order of the tree 2 a node with k children has k1 keys 3 all leaves are at the same depth 4 the root has at least 2 children unless its a leaf 5 nonroot nodes must have at least m2 children 6 keys within a node are sorted in ascending order 7 each key separates child subtrees meaning keys in the left child are less than the key keys in the right child are greater than the key b tree properties 1 internal nodes contain only keys no data 2 leaf nodes store all data records and they are linked in a doubly linked list for fast sequential access 3 the root can be a leaf node if there are few elements 4 every internal node has at least m2 children except the root 5 searches always end at leaf nodes making range queries more efficient 6 insertions deletions and searches take olog_m n time btree vs b tree btrees store both keys and values in internal and leaf nodes b trees store values only in leaf nodes while internal nodes contain only keys for navigation b trees allow efficient range queries because of the linked list of leaves btrees are better for random access while b trees are better for range scans btree insertion 1 start at the root node 2 traverse the tree to the correct leaf 3 insert the key in sorted order 4 if the node overflows exceeds max keys split it the middle key moves up to the parent the node splits into two redistributing keys 5 if the root splits create a new root increasing the tree height example insert 42 into a btree of order 4 m4 1 find the correct position in the leaf node 2 insert 42 3 if the node has too many keys split and move the middle key up btree deletion 1 find the key in the tree 2 if the key is in a leaf node remove it 3 if the key is in an internal node replace it with the predecessor largest key in left subtree or successor smallest key in right subtree 4 if the node has too few keys borrow a key from a sibling or merge nodes example delete 42 from a btree 1 locate 42 2 if its in a leaf remove it 3 if its in an internal node find the successorpredecessor to replace it 4 if a node has too few keys rebalance the tree b tree insertion 1 start at the root node 2 traverse the tree to find the correct leaf node 3 insert the key in sorted order in the leaf 4 if the leaf overflows split the node and copy the smallest key of the new node to the parent instead of moving it 5 if an internal node overflows split it and push the middle key to the parent 6 if the root splits create a new root increasing the tree height example insert 35 into a b tree m4 1 find the correct leaf 2 insert 35 3 if the leaf overflows split it 4 copy the smallest key of the new leaf to the parent b tree deletion 1 find the key in the leaf node 2 remove it from the leaf 3 if the leaf has too few keys borrow a key from a sibling or merge with a sibling 4 if an internal node has too few keys adjust the parent pointers 5 if the root has only one child left it becomes the new root example delete 35 from a b tree 1 locate 35 in the leaf 2 remove it 3 if the node has too few keys borrow from a sibling or merge b tree advantages over btrees 1 faster range queries leaf nodes are linked for sequential access 2 better disk access internal nodes are smaller improving cache performance 3 consistent access time all searches reach the leaf level 4 efficient insertions and deletions rebalancing is straightforward btrees vs b trees summary btrees keys and values stored in internal and leaf nodes random access is fast not ideal for range queries b trees keys in internal nodes data in leaf nodes linked leaf nodes make range queries faster more spaceefficient internal nodes example prompt insert 42 into the following b tree expected response correctly modified tree with a split if necessary what is the inorder traversal of this btree expected response sorted list of keys delete 35 from the given b tree expected response correctly modified tree with adjustments to parent pointers"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -16-20",
            "chunked_content": {
                "200_words_overlap_0": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view",
                    "is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent",
                    "transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and",
                    "other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot",
                    "achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "200_words_overlap_50": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view",
                    "to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of",
                    "unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts",
                    "it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram",
                    "other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot",
                    "nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will",
                    "for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the",
                    "fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "200_words_overlap_100": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view",
                    "straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical",
                    "is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent",
                    "unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts",
                    "transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and",
                    "set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning",
                    "other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot",
                    "distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs",
                    "achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the",
                    "nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements",
                    "right database depends on performance needs scalability and data consistency requirements"
                ],
                "500_words_overlap_0": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts",
                    "set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "500_words_overlap_50": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts",
                    "it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will",
                    "for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "500_words_overlap_100": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts",
                    "transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs",
                    "achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "1000_words_overlap_0": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "1000_words_overlap_50": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ],
                "1000_words_overlap_100": [
                    "03 moving beyond the relational model benefits of the relational model mostly standard data model and query language acid compliance atomicity consistency isolation durability ensures reliable transactions highly structured data support scalability for large amounts of data widespread adoption with extensive tooling and expertise available relational database performance many ways that a rdbms like a supersmart librarian that organizes and speeds up the way we find and store information increases efficiency indexing the topic we focused on instead of flipping through every page to find a word you go to the index at the back find the word and jump straight to the right page directly controlling storage a database carefully manages where and how data is stored on a computer making it quicker to access column oriented storage vs row oriented storage query optimization when you ask a database a question a query it figures out the fastest way to get the answer instead of checking everything one by one cachingprefetching caching saves frequently used data so the database doesnt have to look it up again and again prefetching predicts what data you might need next and loads it early to speed things up materialized views a materialized view is a saved result of a complicated query so the database doesnt have to recalculate it every time someone asks precompiled stored procedures a stored procedure is a saved piece of code that runs quickly when called making repeated tasks faster data replication and partitioning replication copies data across multiple servers so more people can access it quickly partitioning splits data into smaller chunks and stores them separately so searches dont have to go through everything at once transaction processing transaction a sequence of one or more of the crud create read update delete operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity the transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation transactions do not interfere with each other durability once committed changes persist even after system failures isolation concurrency issues problems that arise due to concurrent transactions 1 dirty reads a transaction reads uncommitted changes from another transaction 2 nonrepeatable reads a transaction reads the same data twice but gets different results because another transaction modified it 3 phantom reads a transaction retrieves a set of records but another transaction addsdeletes rows in that set before it finishes example sql transaction money transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where account_id sender_id attempt to credit money to receiver update accounts set balance balance amount where account_id receiver_id check for insufficient funds if select balance from accounts where account_id sender_id 0 then rollback signal sqlstate 45000 set message_text rollback_message else commit select commit_message as result end if end delimiter challenges of the relational model why rdbms may not always be the best choice evolving schemas some applications require flexible structures acid compliance overhead not all applications need strict transaction guarantees expensive joins queries involving multiple tables can slow down performance semistructured unstructured data json xml and other flexible formats are hard to fit into rigid schemas horizontal scaling limitations traditional relational databases struggle with distributed architectures realtime lowlatency applications some use cases require databases optimized for speed over consistency scalability vertical vs horizontal scaling vertical scaling scaling up increases system capacity by upgrading hardware cpu ram ssds pros easier to implement no need to modify software cons expensive and has hardware limitations horizontal scaling scaling out expands capacity by distributing the workload across multiple machines pros more scalable better redundancy lower hardware costs cons requires changes in database architecture introduces complexities like data replication and partitioning distributed databases cap theorem what is a distributed system a collection of independent computers that appear as one system to the user key characteristics concurrent operations across multiple machines independent failures one machine can crash without affecting others no shared global clock distributed data stores data is stored on multiple nodes and often replicated examples relational mysql postgresql with sharding replication nosql mongodb cassandra cockroachdb network partitioning is inevitable failures can occur at any time systems must be designed to handle partition tolerance p gracefully cap theorem tradeoffs in distributed databases the cap theorem states that a distributed system cannot achieve all three of the following properties simultaneously 1 consistency c every read gets the most recent write or an error in other words all nodes always return the same uptodate data 2 availability a every request receives a response but the response may not contain the most recent data the system remains operational even if some nodes fail 3 partition tolerance p the system continues operating despite network failures that separate nodes into isolated groups understanding cap theorem in practice consistency and availability ca a system that guarantees both consistency and availability will fail if a network partition occurs for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures",
                    "for example traditional relational databases running on a single machine provide strong consistency and availability but cannot tolerate network failures across multiple servers consistency and partition tolerance cp a system that prioritizes consistency and partition tolerance will sacrifice availability this means that if a network partition occurs some requests will be rejected to ensure that all nodes return the most uptodate data google spanner is an example of a cp system availability and partition tolerance ap a system that prioritizes availability and partition tolerance may return outdated eventually consistent data but will always respond to requests even during network failures nosql databases like cassandra and dynamodb follow this model since network partitions are inevitable in distributed systems databases must make tradeoffs between consistency c and availability a based on their use case requirements realworld implications of cap theorem distributed databases must choose tradeoffs based on application needs no system is fully cp ca or ap tradeoffs depend on workload demands example google spanner prioritizes c p strong consistency partitions tolerated cassandra prioritizes a p high availability eventual consistency key takeaways relational databases excel at structured highintegrity data storage distributed systems introduce tradeoffs between consistency availability and partition tolerance choosing the right database depends on performance needs scalability and data consistency requirements"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -21-25",
            "chunked_content": {
                "200_words_overlap_0": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic",
                    "limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb",
                    "3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may",
                    "result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute",
                    "after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "200_words_overlap_50": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic",
                    "data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader",
                    "subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside",
                    "read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage",
                    "result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute",
                    "a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data",
                    "see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "200_words_overlap_100": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic",
                    "reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a",
                    "limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb",
                    "subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside",
                    "3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may",
                    "may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on",
                    "result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute",
                    "the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always",
                    "after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data",
                    "see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "500_words_overlap_0": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside",
                    "may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "500_words_overlap_50": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside",
                    "read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data",
                    "see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "500_words_overlap_100": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside",
                    "3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always",
                    "after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "1000_words_overlap_0": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "1000_words_overlap_50": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ],
                "1000_words_overlap_100": [
                    "04 data replication why replicate data data replication involves storing copies of the same data on multiple machines to enhance performance availability and reliability distributed systems commonly use replication to manage high data loads efficiently benefits of data replication 1 scalability high throughput as data volume and readwrite requests grow a single machine cannot handle the load replication allows multiple machines to distribute the workload 2 fault tolerance high availability if a machine fails another machine with a copy of the data can take over ensuring system reliability 3 low latency for global users replicating data across geographically distributed servers reduces access time for users worldwide challenges of distributed data while replication offers advantages it introduces several challenges consistency changes made on one machine must be propagated across the network to all copies application complexity managing reads and writes across multiple replicated nodes often falls to the application partitioning ensuring data is properly segmented without conflicts scaling strategies vertical vs horizontal vertical scaling scaling up uses a single powerful machine with more cpu memory or disk space some fault tolerance exists through hotswappable components replaceable parts without shutting down the system example a single highend database server handling increasing traffic limitations expensive hardware limitations and a single point of failure horizontal scaling scaling out uses multiple machines each with its own cpu memory and disk coordination happens via the application layer over a network geographically distributed improving reliability and performance example a nosql database cluster like mongodb or cassandra where each node handles a portion of the data benefits better fault tolerance failure of one machine doesnt bring down the system easier cost management commodity hardware is cheaper than highend servers replication vs partitioning replication multiple machines store the same copy of the data partitioning each machine stores only a subset of the data example a global ecommerce platform might replicate customer accounts across all servers for high availability while partitioning order history by region common replication strategies distributed databases typically use one of the following replication models 1 single leader model all write operations go to a single leader node the leader sends replication updates to follower nodes clients can read from either the leader or followers example mysql postgresql sql server 2 multiple leader model multiple nodes act as leaders allowing writes on different nodes requires conflict resolution strategies if nodes receive conflicting updates example google spanner dynamodb 3 leaderless model any node can receive write requests conflict resolution happens during reads eventual consistency example amazon dynamodb cassandra leaderbased replication how it works 1 clients send write requests only to the leader 2 the leader sends replication updates to followers 3 followers apply the updates 4 clients can read data from either the leader or followers common implementations relational databases mysql oracle sql server nosql databases mongodb rethinkdb for realtime apps espresso linkedin messaging brokers kafka rabbitmq how replication information is transmitted replication can be performed using different methods statementbased replication sends sql statements insert update delete downside may cause errors with nondeterministic functions eg now writeahead log wal replication copies bytelevel changes downside requires all nodes to use the same storage engine logical rowbased replication transfers only modified rows advantage decoupled from storage engine making parsing easier triggerbased replication uses database triggers to log changes advantage applicationspecific flexibility but can introduce overhead synchronous vs asynchronous replication synchronous replication leader waits for a response from followers before committing changes ensures strong consistency every node has the latest data downside slower writes due to waiting asynchronous replication leader does not wait for confirmation from followers faster writes but may result in temporary inconsistencies eventual consistency advantage better performance suitable for highavailability applications handling leader failures when a leader fails a new leader must be chosen challenges include selecting a new leader use a consensus algorithm eg nodes vote for the most uptodate node assign a controller node to manage leadership changes ensuring data consistency if using asynchronous replication some writes may be lost strategies include replaying logs or discarding lost writes preventing splitbrain scenarios if multiple nodes act as leaders conflicting writes may occur requires conflict resolution mechanisms replication lag replication lag is the delay between a write on the leader and the update appearing on followers synchronous replication minimal lag but slower writes as followers must confirm every update asynchronous replication higher lag but better performance eventual consistency the delay in consistency caused by replication lag is known as the inconsistency window readafterwrite consistency scenario you comment on a reddit post after submitting you expect your comment to be visible immediately but its less critical for others to see it instantly methods to ensure readafterwrite consistency 1 always read recent updates from the leader 2 dynamically switch to leader reads for recently updated data eg for 1 minute after an update monotonic read consistency issue if a user reads data from multiple followers they might see older data after reading newer data solution monotonic read consistency ensures that users never see outdated data after previously reading updated data consistent prefix reads issue if different partitions replicate data at different speeds reads may return outoforder updates solution consistent prefix reads guarantee that updates appear in the correct order maintaining logical consistency example a user posts comment a then comment b if replication delays occur followers might see comment b before a causing confusion with consistent prefix reads users always see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data",
                    "see a before b as intended key takeaways replication improves availability fault tolerance and scalability different replication models balance consistency performance and complexity leader failures require careful handling to maintain data integrity read consistency mechanisms prevent users from seeing outdated or outoforder data"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -26-31",
            "chunked_content": {
                "200_words_overlap_0": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational",
                    "databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue",
                    "stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key",
                    "no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in",
                    "order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "200_words_overlap_50": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational",
                    "databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database",
                    "system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently",
                    "queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name",
                    "no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in",
                    "for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic",
                    "various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "200_words_overlap_100": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational",
                    "safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the",
                    "databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue",
                    "system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently",
                    "stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key",
                    "user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological",
                    "no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in",
                    "order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports",
                    "order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic",
                    "various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "500_words_overlap_0": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently",
                    "user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "500_words_overlap_50": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently",
                    "queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic",
                    "various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "500_words_overlap_100": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently",
                    "stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports",
                    "order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "1000_words_overlap_0": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "1000_words_overlap_50": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ],
                "1000_words_overlap_100": [
                    "05 nosql keyvalue databases distributed databases and acid pessimistic concurrency acid transactions are used in traditional databases to ensure data safety they follow a pessimistic concurrency model assuming that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete write lock analogy borrowing a book from a library if you have it no one else can until you return it optimistic concurrency unlike pessimistic concurrency transactions do not lock data when they read or write this method assumes conflicts are unlikely to occur and even if they do they can be handled safely how it works each table has a last update timestamp and version number before committing a change the system checks if another transaction has modified the data if so the transaction is rolled back and retried optimistic vs pessimistic concurrency optimistic concurrency is better for lowconflict systems eg analytical databases backups pessimistic concurrency is better for highconflict systems where frequent transaction rollbacks would be inefficient nosql introduction the term nosql was first used in 1998 by carlo strozzi for a relational database system that did not use sql today nosql means not only sql and often refers to nonrelational databases nosql databases emerged to handle unstructured webbased data at scale cap theorem a distributed database can only guarantee two out of the three 1 consistency every user sees the same data at any given time 2 availability the system remains operational even if some components fail 3 partition tolerance the system can function even when network partitions occur cap tradeoffs consistency availability the system always returns uptodate data but it may not handle network failures well consistency partition tolerance guarantees latest data across a distributed system but requests may be dropped if a partition occurs availability partition tolerance the system always responds but data might be slightly outdated base an alternative to acid for distributed systems base is a more flexible model for handling distributed databases trading strict consistency for performance basically available ensures high availability even if the response might not always be reliable soft state the database state may change over time without new inputs due to eventual consistency eventual consistency given enough time all nodes will converge to the same data state keyvalue databases kv dbs a keyvalue store is the simplest form of nosql database storing data as a key value pair why use keyvalue stores simplicity the data model is very basic compared to relational tables speed often deployed as inmemory databases where retrieval is an o1 operation using hash tables scalability designed for horizontal scaling allowing more machines to be added easily tradeoffs nosql kv stores are great for speed but lack complex queries or relationships between data use cases for keyvalue stores data science machine learning use cases edaexperimentation store intermediate results from data preprocessing feature store store frequently accessed ml features for lowlatency retrieval model monitoring track realtime model performance metrics software engineering use cases session storage store session data efficiently user profiles preferences store userspecific settings with fast retrieval shopping carts maintain cart data across devices and sessions caching layer improve performance by storing frequently accessed data inmemory redis a popular keyvalue store redis remote dictionary server is a widely used opensource inmemory keyvalue store features of redis supports various data structures beyond simple keyvalue pairs such as lists sets hashes and sorted sets durability options snapshotting saves a full copy to disk periodically appendonly file aof logs every change for recovery extremely fast handles 100000 operations per second limitations no complex queries or relationships only supports lookups by key no secondary indexes redis data types string type simplest type stores a single value under a key use cases caching htmlcssjs fragments storing user settings and configuration counting web page views or rate limiting hash type stores multiple fieldvalue pairs under one key use cases user profiles eg store name email preferences session tracking eg active login data example commands hset user100 name alice age 25 email aliceexamplecom hget user100 name hgetall user100 redis lists a list in redis is a linked list of string values use cases implementing queues and stacks social media feeds eg storing posts in chronological order chat message history batch processing queueing tasks for later execution example commands lpush messages hello lpush messages how are you rpop messages retrieves hello redis sets a set in redis is an unordered collection of unique strings use cases tracking unique visitors to a website managing access control lists for users storing friendships and group memberships in social networks performing set operations like intersection and difference example commands sadd usersonline alice bob charlie sismember usersonline alice returns true scard usersonline counts members in set redis sorted sets a sorted set is similar to a set but keeps elements in order based on a score use cases leaderboards eg sorting players by score task scheduling eg prioritizing jobs ranking systems eg most popular articles example commands zadd leaderboard 100 alice 150 bob zrange leaderboard 0 1 withscores json support in redis redis supports storing json documents uses jsonpath syntax for querying stored in a binary tree structure for fast access to subelements example command jsonset user1 name alice age 25 jsonget user1 final takeaways nosql databases are designed for flexibility scalability and speed keyvalue stores provide fast lookups but lack complex querying redis is an efficient keyvalue store that supports various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic",
                    "various data structures choosing the right concurrency model depends on the systems needs optimistic vs pessimistic"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -32-34",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all",
                    "keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a",
                    "subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include",
                    "save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "200_words_overlap_50": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all",
                    "data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key",
                    "hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the",
                    "which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install",
                    "save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "200_words_overlap_100": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all",
                    "redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a",
                    "keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a",
                    "hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the",
                    "subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include",
                    "score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval",
                    "save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications",
                    "and scalability in modern applications"
                ],
                "500_words_overlap_0": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the",
                    "score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "500_words_overlap_50": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the",
                    "which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "500_words_overlap_100": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the",
                    "subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "1000_words_overlap_0": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "1000_words_overlap_50": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ],
                "1000_words_overlap_100": [
                    "redis a brief history and important commands redis remote dictionary server is an opensource inmemory data structure store designed for highperformance applications it was created in 2009 by salvatore sanfilippo to improve the scalability of his startups database initially developed as a caching layer redis evolved into a fullfeatured keyvalue store used for realtime applications caching messaging and session management in 2010 redis 20 introduced persistence features like rdb and aof along with publishsubscribe messaging in 2013 redis 26 added lua scripting for atomic operations in 2015 redis 30 introduced redis cluster enabling horizontal scaling across multiple nodes in 2018 redis 50 introduced streams allowing realtime data processing in 2021 redis became opensource under a bsd license with redis enterprise providing commercial support today redis is used by major companies like twitter netflix and github for highspeed data storage and retrieval important redis commands include those for basic database management data manipulation expiration control transactions messaging and pipelines for batch execution basic commands include ping which checks if redis is running select db_number which switches between databases default is 0 info which displays redis server statistics flushdb which clears all keys in the current database and flushall which clears all keys in all databases string commands include set key value which stores a value under a key get key which retrieves a value del key which deletes a key incr key which increments a number stored in a key and expire key seconds which sets a timetolive ttl for a key additional commands include mset to set multiple keyvalue pairs at once and mget to retrieve multiple values example commands for strings include redis_clientsetclickcountabc 0 redis_clientincrclickcountabc printredis_clientgetclickcountabc returns 1 hash commands include hset key field value which stores a fieldvalue pair hget key field which retrieves a field from a hash and hgetall key which retrieves all fieldvalue pairs additional commands include hkeys to get all fields hlen to get the number of fields and hdel to delete a field example commands for hashes include redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 printredis_clienthgetallusersession123 list commands include lpush key value which adds a value to the left of the list rpush key value which adds a value to the right of the list lpop key which removes and returns the first element rpop key which removes and returns the last element and lrange key start stop which retrieves a subset of the list additional commands include llen to get the list length lset to modify an existing value and lrem to remove specific elements set commands include sadd key value which adds a value to a set smembers key which retrieves all values in a set srem key value which removes a value from the set and scard key which returns the number of elements in the set sorted set commands include zadd key score value which adds a value with a score zrange key start stop which retrieves values in order and zscore key value which gets the score of a value expiration and transaction commands include expire key seconds which sets an expiration time ttl key which checks the remaining timetolive multi which starts a transaction and exec which executes all queued commands in a transaction pipelines help reduce network overhead by batching multiple redis commands into a single request improving performance in highthroughput applications example pipeline usage pipe redis_clientpipeline for i in range5 pipesetfseati fi pipeexecute pubsub commands for messaging include publish channel message which sends a message to a channel and subscribe channel which listens for messages on a channel persistence and backup commands include save which manually saves data to disk bgsave which saves data in the background and lastsave which returns the timestamp of the last save redis can also be integrated with python using the redispy client which allows python applications to interact with redis databases to install it use pip install redis connecting to a redis server in python can be done with import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis is one of the fastest and most efficient inmemory databases widely used for caching realtime analytics machine learning feature stores and session storage its command set enables highspeed data retrieval and scalability in modern applications"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -35-37",
            "chunked_content": {
                "200_words_overlap_0": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents",
                    "can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "200_words_overlap_50": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents",
                    "column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb",
                    "with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "200_words_overlap_100": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents",
                    "collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting",
                    "can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries",
                    "with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "500_words_overlap_0": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "500_words_overlap_50": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "500_words_overlap_100": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "1000_words_overlap_0": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "1000_words_overlap_50": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ],
                "1000_words_overlap_100": [
                    "07 document databases and mongodb history of mongodb mongodb was founded in 2007 by engineers from doubleclick acquired by google who realized relational databases were not efficient for handling massivescale data such as serving 400000 ads per second the name mongodb was derived from humongous database mongodb atlas the fully managed cloud version was introduced in 2016 mongodb data structure mongodb is structured hierarchically database contain multiple collections collection group of related documents analogous to tables in relational databases document is a single jsonlike object stored within a collection relational vs mongodb comparison in relational databases a database is a collection of related tables whereas in mongodb a database contains multiple collections a table or view in a relational database is equivalent to a collection in mongodb a row in a relational database corresponds to a document in mongodb where each document is stored in json or bson format a column in a relational database is similar to a field in mongodb representing an individual data attribute an index functions the same in both relational databases and mongodb optimizing query performance a foreign key in relational databases is used to establish relationships between tables whereas in mongodb references between documents can be created using embedded documents or manual references instead of performing joins like in relational databases mongodb uses embedded documents where related data is stored within a single document for faster retrieval mongodb features rich query support supports all crud create read update delete operations indexing supports primary and secondary indices for faster querying replication supports replica sets for high availability with automatic failover load balancing builtin support for distributing load across multiple nodes mongodb deployment options mongodb atlas fully managed cloudbased mongodb dbaas mongodb enterprise subscriptionbased selfmanaged version with enterprise features mongodb community opensource freetouse selfmanaged version interacting with mongodb mongodb tools mongosh mongodb shell cli tool for interacting with mongodb instances mongodb compass free opensource gui tool for managing mongodb databases datagrip thirdparty tools additional guibased database management tools mongodb libraries for programming languages pymongo python mongoose javascriptnodejs other libraries for java c go etc running mongodb in docker create a container map host port to container port 2701 set an initial username and password for the superuser using mongodb compass mongodb compass provides a visual interface for working with mongodb it allows users to connect to a database create collections and run queries"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -38-42",
            "chunked_content": {
                "200_words_overlap_0": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb",
                    "from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter",
                    "notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "200_words_overlap_50": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb",
                    "or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document",
                    "pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications",
                    "way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "200_words_overlap_100": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb",
                    "an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install",
                    "from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter",
                    "pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications",
                    "notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "500_words_overlap_0": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "500_words_overlap_50": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications",
                    "way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "500_words_overlap_100": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications",
                    "notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "1000_words_overlap_0": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "1000_words_overlap_50": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ],
                "1000_words_overlap_100": [
                    "mongodb commands queries basic mongodb shell commands show all databases show dbs use a specific database use mydatabase show all collections in the current database show collections crud operations in mongodb creating a database and collection create a new database and collection use mflix dbcreatecollectionusers inserting documents insert a single document dbusersinsertone name john doe age 30 email johnexamplecom insert multiple documents dbusersinsertmany name alice age 25 name bob age 28 finding documents find all documents equivalent to select from users in sql dbusersfind find documents with a specific condition dbusersfind name davos seaworth find movies released in mexico with an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 find movies from 2010 that won at least 5 awards or belong to the drama genre dbmoviesfind year 2010 or awardswins gte 5 genres drama counting documents count how many movies match a query dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama projecting specific fields return only the name field of movies that match a query dbmoviesfind year 2010 or awardswins gte 5 genres drama name 1 _id 0 mongodb with python pymongo connecting to mongodb in python install pymongo pip install pymongo connect to mongodb from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 db clientds4300 collection dbmycollection performing crud operations in pymongo insert a document post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id count documents in a collection collectioncount_documents mongodb provides a powerful and flexible approach to handling jsonlike data making it an essential tool for modern applications that require scalable and efficient data storage more on mongodb pymongo using pymongo to connect to mongodb pymongo is a python library used to interface with mongodb databases it allows applications to perform crud create read update delete operations on mongodb collections to install pymongo use pip install pymongo to connect to a mongodb instance running locally from pymongo import mongoclient client mongoclientmongodbuser_namepasswordlocalhost27017 getting a database and collection once connected you can access a database and its collections db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document to insert a document into a collection post author mark text mongodb is cool tags mongodb python post_id collectioninsert_onepostinserted_id printpost_id prints the inserted documents unique id querying data to find all movies released in the year 2000 from bsonjson_util import dumps movies_2000 dbmoviesfindyear 2000 printdumpsmovies_2000 indent2 pretty print the results using pymongo in jupyter notebooks to use pymongo in a jupyter notebook activate your python virtual environment conda or venv install dependencies pip install pymongo jupyterlab download and unzip the sample jupyter notebooks from the provided link navigate to the folder where you unzipped the files and run jupyter lab pymongo provides a flexible way to interact with mongodb allowing efficient storage and retrieval of jsonlike documents in python applications"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -4-7",
            "chunked_content": {
                "200_words_overlap_0": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key",
                    "is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6",
                    "9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6",
                    "from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "200_words_overlap_50": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key",
                    "selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif",
                    "with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues",
                    "postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15",
                    "from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "200_words_overlap_100": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key",
                    "the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node",
                    "is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6",
                    "with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues",
                    "9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6",
                    "inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15",
                    "from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "500_words_overlap_0": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues",
                    "inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "500_words_overlap_50": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues",
                    "postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "500_words_overlap_100": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues",
                    "9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "1000_words_overlap_0": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "1000_words_overlap_50": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ],
                "1000_words_overlap_100": [
                    "binary search tree bst guide binary search tree bst guide a binary search tree bst is a binary tree where the left child of a node contains a key less than the nodes key the right child of a node contains a key greater than the nodes key no duplicate keys exist in the tree each node in a bst consists of key the value stored in the node left a reference to the left child right a reference to the right child bst insertion to insert a key into a bst 1 start at the root node 2 if the key is smaller than the current node move to the left subtree 3 if the key is larger move to the right subtree 4 repeat until a nil empty position is found then insert the new node example insertion function class treenode def __init__self key selfkey key selfleft none selfright none def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key return root bst search to search for a key in a bst 1 start at the root 2 if the key matches return the node 3 if the key is smaller search in the left subtree 4 if the key is larger search in the right subtree 5 if the key is not found return none example search function def searchroot key if root is none or rootkey key return root if key rootkey return searchrootleft key return searchrootright key bst deletion to delete a node with key k 1 if the node has no children remove it 2 if the node has one child replace it with its child 3 if the node has two children find the successor smallest node in the right subtree replace the node with its successor delete the successor from its original position example deletion function def find_minnode while nodeleft node nodeleft return node def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey return root bst traversal methods inorder traversal left root right visits nodes in ascending order for a bst def inorderroot if root inorderrootleft printrootkey end inorderrootright example output for a bst with nodes 7 4 12 2 6 9 15 the inorder traversal prints 2 4 6 7 9 12 15 preorder traversal root left right used for copying a tree def preorderroot if root printrootkey end preorderrootleft preorderrootright postorder traversal left right root used for deleting a tree deletes children before the parent def postorderroot if root postorderrootleft postorderrootright printrootkey end building a bst from a list if you provide a list of values you can construct a bst dynamically example function def build_bst_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_bst_from_listvalues inorderroot output 2 5 7 10 12 15 18 bst rules for llm completion to ensure the llm can complete a bst when given an incomplete tree it should learn insertion patterns from partial trees understand traversal outputs and expected orders recognize bst properties ensuring left children are smaller and right children are larger prompt examples given the bst 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -43-45",
            "chunked_content": {
                "200_words_overlap_0": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships",
                    "but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding",
                    "algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing",
                    "can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "200_words_overlap_50": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships",
                    "a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs",
                    "graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting",
                    "explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection",
                    "can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "200_words_overlap_100": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships",
                    "on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected",
                    "but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding",
                    "graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting",
                    "algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing",
                    "closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection",
                    "can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "500_words_overlap_0": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting",
                    "closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "500_words_overlap_50": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting",
                    "explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "500_words_overlap_100": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting",
                    "algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "1000_words_overlap_0": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "1000_words_overlap_50": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ],
                "1000_words_overlap_100": [
                    "graph databases graph theory what is a graph database a graph database is a type of database designed to store and manage data using the graph data structure which consists of nodes vertices represent entities eg people locations products edges define relationships between nodes eg friend_of works_at properties attributes associated with nodes or edges eg name age weight graph databases allow for efficient graphbased queries such as traversals and shortest path calculations making them suitable for applications involving complex relationships where do graphs appear graphs are commonly used in many fields including social networks platforms like instagram and facebook rely on graphs to model relationships between users the web the internet is essentially a massive graph of web pages nodes connected by hyperlinks edges biological and chemical data graphs are used in genetics and molecular biology to model interactions between genes and proteins basics of graph theory labeled property graphs a labeled property graph consists of nodes vertices represent individual entities edges relationships define connections between nodes labels categorize nodes into groups eg person car properties store keyvalue kv attributes on nodes and edges example node labels person car relationship types drives owns lives_with married_to nodes can exist without relationships but edges must always connect two nodes paths in a graph a path is an ordered sequence of nodes connected by edges where no nodes or edges repeat example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats types of graphs connected vs disconnected graphs connected graph there is a path between any two nodes disconnected graph some nodes are not connected to the rest of the graph weighted vs unweighted graphs weighted graph each edge has a numerical weight eg distance cost unweighted graph all edges are treated equally directed vs undirected graphs directed graph edges have a direction eg follows on social media undirected graph edges do not have direction eg friends with cyclic vs acyclic graphs cyclic graph contains at least one cycle a path that returns to the same node acyclic graph no cycles exist sparse vs dense graphs sparse graph few edges relative to the number of nodes dense graph many edges relative to the number of nodes trees special type of graphs a tree is a type of acyclic and connected graph where there is exactly one path between any two nodes types of graph algorithms pathfinding algorithms pathfinding determines the shortest path between two nodes shortest path minimizing the number of edges or the total weight of a path average shortest path measures efficiency and resiliency of networks other pathfinding algorithms minimum spanning tree cycle detection maxmin flow breadthfirst search bfs vs depthfirst search dfs bfs explores neighbors first before moving deeper good for shortest paths dfs explores deep into one branch before backtracking good for pathfinding in mazes centrality community detection algorithms centrality identifies important nodes in a network example finding social media influencers community detection identifies clusters or partitions in a graph example detecting closeknit groups in social networks famous graph algorithms dijkstras algorithm finds the shortest path in graphs with positive edge weights a algorithm similar to dijkstras but incorporates heuristics to speed up searches pagerank measures the importance of nodes based on incoming links used by google search neo4j a graph database neo4j is a leading graph database system that supports both transactional and analytical processing of graphbased dat features of neo4j schemaoptional does not require a predefined schema but allows one if needed acid compliance supports atomicity consistency isolation and durability indexing allows efficient retrieval of nodes and edges distributed computing can scale horizontally across multiple machines other graph databases microsoft cosmos db amazon neptune graph databases are useful for applications where relationships between data points are as important as the data itself such as social networks recommendation systems and fraud detection"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -46-51",
            "chunked_content": {
                "200_words_overlap_0": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace",
                    "neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv",
                    "file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing",
                    "routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "200_words_overlap_50": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace",
                    "named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by",
                    "use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector",
                    "avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with",
                    "routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "200_words_overlap_100": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace",
                    "language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead",
                    "neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv",
                    "use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector",
                    "file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing",
                    "as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management",
                    "routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "500_words_overlap_0": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector",
                    "as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "500_words_overlap_50": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector",
                    "avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "500_words_overlap_100": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector",
                    "file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "1000_words_overlap_0": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "1000_words_overlap_50": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ],
                "1000_words_overlap_100": [
                    "neo4j a graph database system what is neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data it belongs to the nosql database family and is used to efficiently store and query data with complex relationships key features of neo4j schemaoptional does not require a predefined schema but allows one if needed supports indexing improves query performance by creating indexes on node properties acid compliant ensures data integrity by following atomicity consistency isolation and durability principles distributed computing support scales horizontally across multiple machines popular alternatives microsoft cosmos db amazon neptune neo4j query language cypher cypher is neo4js graph query language introduced in 2011 it provides a structured and visual way to query graph databases similar to sql for relational databases basic cypher syntax nodes are enclosed in parentheses nlabel relationships are represented with relationship personfriend_withotherperson example create a node representing a user named alice create user name alice birthplace paris add a relationship between alice and bob match aliceuser name alice match bobuser name bob create aliceknows since 20221201bob note relationships in neo4j are directed querying data in neo4j find all users born in london match usruser birthplace london return usrname usrbirthplace neo4j plugins apoc awesome procedures on cypher plugin an addon library providing hundreds of procedures and functions enhances data importexport graph algorithms and query optimizations graph data science plugin provides efficient implementations of graph algorithms like shortest path centrality and community detection running neo4j with docker compose docker compose is a tool for managing multiple containers making it easier to set up neo4j in a reproducible way basic dockercomposeyaml configuration for neo4j yaml copyedit services neo4j container_name neo4j image neo4jlatest ports 74747474 76877687 environment neo4j_authneo4jneo4j_password neo4j_pluginsapoc graphdatascience volumes neo4j_dbdatadata neo4j_dbimportvarlibneo4jimport important never store secrets eg passwords directly in dockercomposeyaml instead use env files to store environment variables docker commands for neo4j start neo4j docker compose up d stop neo4j docker compose down rebuild without cache docker compose build nocache working with neo4j browser access the neo4j browser at localhost7474 where you can enter cypher queries example checking movies directed by a person match mmovie title raydirectedpperson return m p importing data into neo4j downloading a dataset to practice with real data download a dataset clone the dataset repository git clone httpsgithubcompacktpublishinggraphdatasciencewithneo4j 1 2 extract netflix_titlescsv and place it in the neo4j_dbimport folder basic csv import in neo4j load a csv file and create movie nodes cypher copyedit load csv with headers from filenetflix_titlescsv as line create movie id lineshow_id title linetitle releaseyear linerelease_year loading csv data general syntax cypher copyedit load csv with headers from filefile_in_import_foldercsv as line fieldterminator specify delimiter if needed perform operations using line importing directors and avoiding duplicates basic approach creates duplicate nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name create person name trimdirector_name better approach avoids duplicates using merge match pperson delete p remove existing nodes load csv with headers from filenetflix_titlescsv as line with splitlinedirector as directors_list unwind directors_list as director_name merge person name director_name creating relationships edges between nodes create a relationship between person and movie load csv with headers from filenetflix_titlescsv as line match mmovie id lineshow_id with m splitlinedirector as directors_list unwind directors_list as director_name match pperson name director_name create pdirectedm neo4j use cases neo4j is widely used in applications where relationships between data points are crucial common use cases social networks friendships and interactions between users recommendation systems suggesting movies books or products based on user preferences fraud detection identifying fraudulent transactions by analyzing transaction relationships network and it management optimizing routing in computer networks summary neo4j is a powerful graph database for storing and querying highly connected data cypher is neo4js query language allowing sqllike queries for graphs docker compose simplifies neo4j deployment and environment variables should be managed via env files csv data can be imported into neo4j with proper handling to avoid duplicate nodes neo4j is widely used in social networks recommendation engines fraud detection and it management"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -52-57",
            "chunked_content": {
                "200_words_overlap_0": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications",
                    "provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services",
                    "database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront",
                    "costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used",
                    "for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such",
                    "as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "200_words_overlap_50": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications",
                    "networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers",
                    "centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents",
                    "integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides",
                    "costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used",
                    "instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda",
                    "key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based",
                    "lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and",
                    "as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway",
                    "applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "200_words_overlap_100": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications",
                    "expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data",
                    "provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services",
                    "centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents",
                    "database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront",
                    "translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3",
                    "costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used",
                    "rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a",
                    "for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based",
                    "write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such",
                    "on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications",
                    "as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway",
                    "and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "500_words_overlap_0": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents",
                    "translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "500_words_overlap_50": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents",
                    "integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda",
                    "key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway",
                    "applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "500_words_overlap_100": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents",
                    "database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a",
                    "for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications",
                    "as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "1000_words_overlap_0": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "1000_words_overlap_50": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ],
                "1000_words_overlap_100": [
                    "aws introduction what is aws amazon web services aws is the leading cloud computing platform offering over 200 services for computing storage databases machine learning and more key features of aws global availability aws has regions and availability zones worldwide to ensure reliability payasyouuse model users pay only for what they consume theoretically making it cheaper than traditional data centers scalability flexibility services scale automatically based on demand security compliance aws ensures high security with tools for encryption identity management and monitoring history of aws 2006 aws launched with two services s3 simple storage service ec2 elastic compute cloud 2010 expansion to include simpledb elastic block store ebs rds relational database service dynamodb cloudfront cloudwatch and more continuous growth aws frequently introduces new services for cloud computing machine learning and analytics cloud computing models cloud services fall into three categories infrastructure as a service iaas provides virtual machines storage and networking users manage the os applications and configurations example aws ec2 elastic compute cloud platform as a service paas provides a managed environment to develop run and manage applications users focus on application development without worrying about infrastructure example aws elastic beanstalk software as a service saas fully managed applications provided by a vendor users only interact with the frontend application example google drive dropbox or aws workspaces aws shared responsibility model aws follows a shared responsibility model for security aws responsibilities security of the cloud securing physical infrastructure data centers and hardware maintaining hypervisors host operating systems and managed services client responsibilities security in the cloud configuring identity and access management iam protecting and encrypting data stored in aws managing applications and network security within their virtual private cloud vpc aws global infrastructure aws ensures availability and performance using regions geographic areas eg useast1 uswest1 availability zones azs data centers within regions to ensure redundancy edge locations used for content delivery networks cdn to cache data closer to users aws core services compute services ec2 elastic compute cloud virtual machines for running applications ec2 spot instances discounted ec2 instances for flexible workloads ecs elastic container service manages docker containers eks elastic kubernetes service manages kubernetes clusters aws lambda serverless computingruns code without provisioning servers storage services s3 simple storage service object storage highly scalable ebs elastic block store block storage for ec2 instances efs elastic file system serverless scalable file storage aws backup automates data backup across aws services database services relational databases amazon rds amazon aurora keyvalue stores amazon dynamodb inmemory databases amazon elasticache amazon memorydb document databases amazon documentdb mongodb compatible graph databases amazon neptune analytics services athena sqlbased querying for data in s3 emr elastic mapreduce big data analytics using spark hive and presto glue data integration preparation and etl extract transform load redshift data warehousing for analytical processing kinesis realtime data streaming and processing quicksight cloudbased business intelligence bi tool machine learning ai services sagemaker endtoend machine learning platform comprehend natural language processing nlp rekognition image and video analysis textract extracts text from scanned documents translate machine translation aws for data analytics engineering aws provides several key tools for data professionals ec2 lambda compute services for processing workloads s3 scalable object storage for big data rds dynamodb relational and nosql databases glue athena data transformation and querying emr managed big data frameworks redshift cloudbased data warehouse for structured analytics aws free tier the aws free tier allows users to explore aws services for 12 months with limits ec2 750 hoursmonth limited instance types s3 5gb of storage 20000 get requestsmonth rds 750 hoursmonth of database use aws free tier enables users to experiment without upfront costs making it great for learning cloud computing amazon ec2 aws lambda amazon ec2 elastic compute cloud ec2 is a cloudbased virtual computing service that allows users to run applications on virtual machines vms it provides scalable computing capacity with a payasyougo pricing model key features of ec2 ec2 provides elasticity allowing instances to scale up or down dynamically based on demand it offers multiple instance types optimized for general compute memory storage and gpu workloads users can create custom amis amazon machine images to launch instances with predefined software and configurations ec2 integrates with aws services such as s3 rds and lambda ec2 lifecycle when using ec2 users can launch an instance with a chosen configuration start or stop an instance without deleting it reboot an instance without losing data and terminate an instance permanently deleting all associated data unless backed up where can you store data on ec2 instance store is highspeed temporary storage tied to the instance lifecycle elastic block store ebs provides persistent blocklevel storage elastic file system efs is scalable file storage shared across multiple instances amazon s3 is scalable object storage for backup and large datasets common ec2 use cases ec2 is commonly used for web hosting to run websites and applications data processing for handling workloads using python r or spark machine learning for training ml models on gpuoptimized instances and disaster recovery for backing up workloads in case of failure setting up an ec2 instance to set up an ec2 instance users need to launch an instance via the aws management console choose an ami amazon machine image based on the os and software stack select an instance type such as t2micro for freetier eligibility configure networking and security by assigning security groups and ssh keys and connect via ssh using a key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app",
                    "key file and public ip address to install necessary packages on an ec2 instance users should update the system and install software such as python using the following commands sudo apt update sudo apt upgrade sudo apt install python3pip running applications on ec2 for deploying applications users can install miniconda on ec2 to do this they need to download the miniconda installer execute the installation script and verify the installation another use case is deploying a streamlit web app on ec2 to do this users should install dependencies such as streamlit and scikitlearn create a directory for the web app write a python script for the streamlit app and then run the app using the streamlit command aws lambda serverless computing aws lambda is a serverless computing service that runs code without requiring users to manage servers it executes functions only when triggered making it costeffective key features of aws lambda aws lambda is eventdriven and runs in response to aws events such as s3 uploads or api gateway requests it supports multiple programming languages including python nodejs java and go the pricing model is based on execution time rather than infrastructure usage lambda automatically scales up or down based on demand how aws lambda works users upload code via the aws console cli or aws sdk define a trigger event such as api gateway or dynamodb updates and aws lambda executes the function when the trigger occurs the results can be stored in aws services like s3 or dynamodb creating an aws lambda function to create an aws lambda function users need to navigate to aws lambda in the aws console click create function select author from scratch choose a runtime such as python 38 write code in the inline editor or upload a zip file configure triggers such as s3 or api gateway deploy the function and test it example aws lambda function in python import json def lambda_handlerevent context return statuscode 200 body jsondumpshello from aws lambda after writing the function users should deploy it and create a test event to verify execution differences between ec2 and lambda ec2 provides full control over the virtual machine while lambda is a fully managed serverless execution environment ec2 requires manual or autoscaling whereas lambda automatically scales based on demand ec2 pricing is based on running instances while lambda charges based on execution time ec2 is best for persistent applications and databases whereas lambda is ideal for shortlived tasks and eventdriven execution summary amazon ec2 provides virtual machines in the cloud allowing full control over computing resources aws lambda enables serverless execution reducing infrastructure management and costs ec2 is best suited for persistent workloads while lambda is ideal for eventdriven applications both services integrate seamlessly with the aws ecosystem including s3 rds and api gateway"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -58-60",
            "chunked_content": {
                "200_words_overlap_0": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation",
                    "on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none",
                    "do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is",
                    "the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "200_words_overlap_50": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation",
                    "imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of",
                    "refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and",
                    "consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb",
                    "the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25",
                    "example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "200_words_overlap_100": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation",
                    "faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing",
                    "on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none",
                    "refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and",
                    "do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is",
                    "fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte",
                    "the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25",
                    "2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "500_words_overlap_0": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and",
                    "fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "500_words_overlap_50": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and",
                    "consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "500_words_overlap_100": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and",
                    "do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "1000_words_overlap_0": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "1000_words_overlap_50": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ],
                "1000_words_overlap_100": [
                    "questions and answers what is the difference between a list where memory is contiguously allocated and a list where linked structures are used a contiguously allocated list array stores elements in consecutive memory locations allowing fast random access in o1 time but making insertions and deletions in the middle inefficient due to shifting elements a linked list stores elements in nodes with pointers to the next or previous node making insertions and deletions efficient o1 in some cases but requiring on time for random access since traversal is needed when are linked lists faster than contiguouslyallocated lists linked lists are faster when frequent insertions and deletions occur in the middle of the list as no shifting is required they are also better for dynamic memory allocation since arrays require resizing however arrays are still preferable for fast indexing and better cache locality add 23 to the avl tree below what imbalance case is created with inserting 23 30 25 35 20 inserting 23 results in 30 25 35 20 23 the balance factor of 20 becomes 1 and the balance factor of 25 becomes 2 creating an imbalance at 25 this is a leftright lr case requiring a left rotation on 20 followed by a right rotation on 25 why is a b tree better than an avl tree when indexing a large dataset b trees are optimized for diskbased indexing while avl trees are optimized for memorybased indexing b trees have a higher branching factor reducing tree height and minimizing disk io they also store all data in leaf nodes making range queries and sequential scans more efficient avl trees being binary require more nodes and deeper tree structures leading to more frequent disk accesses what is diskbased indexing and why is it important for database systems diskbased indexing refers to storing index structures on disk rather than in memory allowing efficient access to large datasets that exceed ram capacity it is important because databases often store terabytes of data and efficient indexing structures like btrees and b trees reduce disk reads improving query performance in the context of a relational database system what is a transaction a transaction is a sequence of one or more database operations that execute as a single unit of work ensuring consistency isolation and durability succinctly describe the four components of acidcompliant transactions atomicity ensures all operations in a transaction succeed or none do consistency maintains database integrity before and after the transaction isolation prevents transactions from interfering with each other durability ensures committed transactions persist even in system failures why does the cap principle not make sense when applied to a singlenode mongodb instance the cap theorem applies to distributed systems where consistency availability and partition tolerance must be balanced a singlenode mongodb instance does not experience network partitions so it inherently provides both consistency and availability unless it crashes describe the differences between horizontal and vertical scaling horizontal scaling scaling out adds more machines to distribute the load improving redundancy and fault tolerance eg nosql databases like mongodb vertical scaling scaling up adds more resources cpu ram to a single machine which is simpler but limited by hardware constraints eg traditional rdbms briefly describe how a keyvalue store can be used as a feature store a feature store manages machine learning features efficiently keyvalue stores like redis store precomputed feature values for quick retrieval in o1 time example storing user features with user_id as the key and their ml feature set as the value when was redis originally released redis was released in 2009 by salvatore sanfilippo in redis what is the difference between the inc and incr commands inc is not a redis command incr increments the value of a key by 1 example set counter 10 incr counter counter is now 11 what are the benefits of bson over json in mongodb bson binary json is used in mongodb instead of json because it allows faster parsing supports additional data types date decimal128 objectid and provides efficient storage with smaller document sizes and optimized indexing write a mongodb query that returns the titles of all suspense movies released between 2010 and 2015 dbmoviesfind genre suspense release_year gte 2010 lte 2015 title 1 _id 0 this query filters movies where the genre is suspense and the release year is between 2010 and 2015 returning only the title field what does the nin operator mean in a mongo query the nin not in operator filters out documents that contain specified values example dbusersfind age nin 18 21 25 this retrieves all users whose age is not 18 21 or 25"
                ]
            }
        },
        {
            "title": "DS4300 Midterm -8-12",
            "chunked_content": {
                "200_words_overlap_0": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the",
                    "left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after",
                    "deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can",
                    "create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "200_words_overlap_50": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the",
                    "rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1",
                    "def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance",
                    "deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm",
                    "create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary",
                    "7 4 12 2 9 15 rebalanced if necessary"
                ],
                "200_words_overlap_100": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the",
                    "insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x",
                    "left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after",
                    "def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance",
                    "deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can",
                    "1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert",
                    "create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary",
                    "7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "500_words_overlap_0": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance",
                    "1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "500_words_overlap_50": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance",
                    "deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "500_words_overlap_100": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance",
                    "deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "1000_words_overlap_0": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "1000_words_overlap_50": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ],
                "1000_words_overlap_100": [
                    "avl tree guide an avl tree is a selfbalancing binary search tree bst where the height difference balance factor between the left and right subtrees of any node is at most 1 each node in an avl tree has key the value stored in the node left a reference to the left child right a reference to the right child height the height of the node the balance factor of a node is balance factor heightleft subtree heightright subtree a tree is balanced if the balance factor of every node is between 1 and 1 insertion in an avl tree insertion follows standard bst insertion but includes rebalancing if needed steps 1 insert the node like in a bst 2 update the height of each affected node 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to rebalance the tree types of rotations right rotation single rotation ll case performed when inserting into the left subtree of the left child left rotation single rotation rr case performed when inserting into the right subtree of the right child leftright rotation double rotation lr case performed when inserting into the right subtree of the left child first left rotation on the left child then right rotation on the node rightleft rotation double rotation rl case performed when inserting into the left subtree of the right child first right rotation on the right child then left rotation on the node example avl tree insertion code class treenode def __init__self key selfkey key selfleft none selfright none selfheight 1 def get_heightnode return nodeheight if node else 0 def get_balancenode return get_heightnodeleft get_heightnoderight if node else 0 def rotate_righty x yleft t2 xright xright y yleft t2 yheight maxget_heightyleft get_heightyright 1 xheight maxget_heightxleft get_heightxright 1 return x def rotate_leftx y xright t2 yleft yleft x xright t2 xheight maxget_heightxleft get_heightxright 1 yheight maxget_heightyleft get_heightyright 1 return y def insertroot key if root is none return treenodekey if key rootkey rootleft insertrootleft key else rootright insertrootright key rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot rotation cases if balance 1 and key rootleftkey return rotate_rightroot if balance 1 and key rootrightkey return rotate_leftroot if balance 1 and key rootleftkey rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and key rootrightkey rootright rotate_rightrootright return rotate_leftroot return root deletion in avl tree deletion follows bst deletion rules but the tree is rebalanced after deletion steps 1 perform standard bst deletion 2 update the height of affected nodes 3 compute the balance factor 4 if the balance factor is outside the range 111111 perform rotations to restore balance example deletion code def deleteroot key if root is none return root if key rootkey rootleft deleterootleft key elif key rootkey rootright deleterootright key else if rootleft is none return rootright elif rootright is none return rootleft temp find_minrootright rootkey tempkey rootright deleterootright tempkey if root is none return root rootheight 1 maxget_heightrootleft get_heightrootright balance get_balanceroot if balance 1 and get_balancerootleft 0 return rotate_rightroot if balance 1 and get_balancerootleft 0 rootleft rotate_leftrootleft return rotate_rightroot if balance 1 and get_balancerootright 0 return rotate_leftroot if balance 1 and get_balancerootright 0 rootright rotate_rightrootright return rotate_leftroot return root avl tree traversals since an avl tree is a bst with extra balancing traversal methods are the same as in a normal bst inorder traversal left root right returns elements in sorted order preorder traversal root left right used for tree reconstruction postorder traversal left right root used for deleting a tree example traversal function def inorderroot if root inorderrootleft printrootkey end inorderrootright building an avl tree from a list you can create an avl tree dynamically by inserting values from a list def build_avl_from_listvalues root none for value in values root insertroot value return root example usage values 10 5 15 2 7 12 18 root build_avl_from_listvalues inorderroot output 2 5 7 10 12 15 18 avl tree rules for llm completion to ensure the llm can complete an avl tree when given an incomplete structure it should learn insertion balancing patterns from examples understand traversal outputs and expected orders recognize avl properties including balance factors and rotations prompt examples for the llm given the avl tree 10 5 15 insert 7 12 and 18 expected response 10 5 15 7 12 18 with necessary rotations what is the inorder traversal of 7 4 12 2 6 9 15 expected response 2 4 6 7 9 12 15 delete node 6 from 7 4 12 2 6 9 15 expected response 7 4 12 2 9 15 rebalanced if necessary"
                ]
            }
        },
        {
            "title": "DS4300_midterm_notes",
            "chunked_content": {
                "200_words_overlap_0": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at",
                    "least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data",
                    "that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or",
                    "nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a",
                    "geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "200_words_overlap_50": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at",
                    "its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data",
                    "search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select",
                    "or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees",
                    "nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a",
                    "from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window",
                    "reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "200_words_overlap_100": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at",
                    "database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear",
                    "least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data",
                    "search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select",
                    "that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or",
                    "but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra",
                    "nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a",
                    "dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be",
                    "geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window",
                    "reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "500_words_overlap_0": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select",
                    "but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "500_words_overlap_50": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select",
                    "or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window",
                    "reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "500_words_overlap_100": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select",
                    "that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be",
                    "geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "1000_words_overlap_0": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "1000_words_overlap_50": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ],
                "1000_words_overlap_100": [
                    "bst if the node belongs to the left subtree it is less than the parent if the node belongs to the right subtree it is greater than the parent traversal of nodes in bst 1 inorder left subtree current node right subtree 2 preorder current node left subtree right subtree 3 postorder left subtree right subtree current node storagespace cpu registers l1 cache l2 cache ram sddhdd l1 cache and l2 cache are faster than ram because they are closer to the processor ram measured at nanosecond speed level sddhdd lots of storage persistent can survive power cycle but slow database systems minimize hddsdd accesses 64 bit integer 8 bytes to get this value need to go through 2048 byte block size tree 4 x 8 32 bytes if you want to store 7 nodes on the same block its 7 x 32 bytes if you store on separate blocks its 7 x 2048 bytes in a perfect balanced tree you never have to read more than 3 blocks of memory b tree in just 2 levels of a b tree you can store 258 nodes and 256 keys nodes in a b tree are always going to be at least half full indexing data structures allow us to access data that is stored on disk faster than scanning disk linearly hash tables like python dictionary gamma load factor n m where n is the number of inserted values and m is the table size hash function hk k mod m hash function always mods table size this ensures that the indices possible are within the size of the table constant amount of work for any k putting things in the table as long as there are slots requires constant time work faster than avl tree in most occasions linear search keeping insert as constant time as possible good dispersion spread has values out each location can be a python list dont make the whole hash table a python list worst version of dispersion is when you dont spread keys across the table benefits of relational model rdbms standard data model and query language acid compliance works well with highly structured data handles large amounts of data well understood lots of toolingexperience increases efficiency with indexing directly controlling storage column oriented storage versus row oriented storage query optimization caching stores frequently used data in fast memory location prefetching predictsfetches data that might be needed soon materialized views database object that stores results of a query precompiled stored procedures data replication and partitioning transaction processing transaction sequence of 1 or more of the crud operations performed as a single logical unit of work entire sequence succeeds commit entire sequence fails rollback or abort helps ensure data integrity error recovery concurrency control reliable data storage simplified error handling dirty read transaction t1 is able to read a row that has been modified by transaction t2 that hasnt yet executed a commit nonrepeatable read two queries in a single transaction t1 execute select but get different values because another transaction t2 has changed data and committed phantom reads transaction t1 is running and transaction t2 adds or deletes rows from set t1 distributed system a collection of independent computers that appear to its users as 1 computer computers operate concurrently computers fail independently no shared global clock single main node and can either perform replication or sharding sharding splitting large amounts of data into smaller chunks shards and storing across multiple servers data stored on 1 node typically replicated each block of data available on n nodes distributed databases can be relational or nonrelational network partitioning process of dividing a network into smaller subnetworks is inevitable network failures system failures system needs to be partition tolerant partition tolerant system keeps running even with network partitions cap theorem impossible for a distributed data store to simultaneously provide more than 2 out of 3 guarantees consistency every read receives most recent writeerror thrown availability every request receives nonerror response no guarantee that response contains most recent write partition tolerance system can continue to operate despite arbitrary network issues rdbms postgresql mysql are consistent and available mongodb hbase redis are consistent and partition tolerant couchdb cassandra dynamodb are available and partition tolerant a consistent and available system is a system that always responds with the latest data and every request gets a response but may not be able to deal with network partitions a consistent and partition tolerant system is when the system responds with data from the distribution system it is always the latest else data request is dropped an available and partition tolerant system is a system that always sends and responds based on distributed stores but may not be the absolute latest data vertical scaling vertical scaling in shared memory architecture has a geographically centralized server and some fault tolerance via hotswappable components in shared memory architecture there is one shared memory and multiple cpus to vertically scale you would add more cpus vertical scaling in shared disk architecture has machines connected via a fast network contention and overhead of locking limit scalability for highwrite volumes however it is ok for data warehouse applications high read volumes in shared disk architecture there is one diskstorage and multiple servers with their own cpu and memory to vertically scale you would add more servers replication lag time it takes for writes on leader to be reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window",
                    "reflected on all followers in synchronous replication lag causes writes to be slower and system to be more brittle as the number of followers increases in asynchronous replication we maintain availability at the cost of delayed or eventual consistency inconsistency window"
                ]
            }
        },
        {
            "title": "DS4300_midterm_notes2",
            "chunked_content": {
                "200_words_overlap_0": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis",
                    "supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many",
                    "modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection",
                    "dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure",
                    "shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "200_words_overlap_50": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis",
                    "eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson",
                    "is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave",
                    "mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes",
                    "dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure",
                    "explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of",
                    "nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "200_words_overlap_100": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis",
                    "used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash",
                    "supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many",
                    "is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave",
                    "modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection",
                    "dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is",
                    "dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure",
                    "probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other",
                    "shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing",
                    "is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "500_words_overlap_0": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave",
                    "dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "500_words_overlap_50": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave",
                    "mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of",
                    "nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "500_words_overlap_100": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave",
                    "modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other",
                    "shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "1000_words_overlap_0": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "1000_words_overlap_50": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ],
                "1000_words_overlap_100": [
                    "readafterwrite consistency one method for implementing readafterwrite consistency is that modifiable data from the clients perspective is always read from the leader another method for implementing readafterwrite consistency is to dynamically switch to reading from the leader for recently updated data the challenge these methods create is that followers were supposed to be proximal to users but now you have to route requests to distant leaders keyvalue stores keyvalue stores are simple especially in comparison to rdbms retrieving a value in a keyvalue store given its key is typically a o1 operation because hash tables or similar data structures are used under the hood there are no concepts of complex queries or joins that slow things down horizontal scaling is simple in keyvalue stores just add more nodes keyvalue stores are typically concerned with eventual consistency meaning that in a distributed environment the only guarantee is that all nodes will eventually converge on the same value redis redis stands for remote directory server redis is an open source inmemory database sometimes called a data structure store it is primarily a kv store but can be used with other models such as graph spatial full text search vector time series redis supports the durability of data by essentially saving snapshots to disk at specific intervals or appendonly file which is a journal of changes that can be used for rollforward if there is a failure redis only supports lookup by key redis keys are usually strings but can be any binary sequence and its values can be strings lists linked lists sets sorted sets hashes and geospatial data redis provides 16 databases by default numbered 0 to 15 value of kv entry is a collection of fieldvalue pairs can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory stack last in first out queue first in first out redis fully supports json standard internally it is stored in a binary tree structure for fast access to subelements bson bson is a binaryencoded serialization of a jsonlike document structure bson supports extended types not part of basic json eg date binarydata etc bson is lightweight and keeps space overhead to a minimum bson is traversable designed to be easily traversed which is vitally important to a document database bson is efficient encoding and decoding must be efficient supported by many modern programming languages xml xquery is the sql of xml xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data impedance mismatch is when two computerssystems are supposed to work together but have different data models or structures the structure of a document is selfdescribing document databases are well aligned with apps that use jsonxml as a transport layer mongodb commands select from users dbusersfind select from users where name dave dbusersfindname dave movies released in mexico imdb rating at least 7 dbmoviesfindcountries mexico imdbrating gte 7 how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments year 2010 or awardswins gte 5 genres drama name 1 _id 0 where 1 return 0 dont return select count from collection dbcollectioncount_documents graph data model examples of graphs are social networks the web and chemicalbiological data in a labeled property graph labels are used to mark a node as part of a group in a labeled property graph nodes with no associated relationships are okay but edges not connected to nodes are not allowed trees a binary tree has up to two child nodes and no cycles a spanning tree is a subgraph of all nodes but not all relationships of the original graph no cycles pathfinding pathfinding is finding the shortest path between two nodes if one exists and is probably the most common operation for graphs the shortest path means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow are other types of pathfinding breadthfirst search bfs is a graph traversal algorithm that explores all nodes at the current level before moving on to the next level starting from a designated root node and systematically visiting neighbors depthfirst search dfs is a graph traversal algorithm that explores as far as possible along each branch before backtracking often implemented using a stack data structure shortest path is the shortest path between two nodes allpairs shortest paths is optimized calculations for shortest paths from all nodes to all other nodes single source shortest path is the shortest path from a root node to all other nodes minimum spanning tree is the shortest path connecting all nodes features of centrality degree refers to the number of connections if a node is connected to many other nodes it has a high degree betweenness refers to whether a node has control over the flow between nodes and groups closeness refers to nodes that can easily reach all other nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j",
                    "nodes in a graph or subgraph dijkstras algorithm is a singlesource shortest path algorithm for positively weighted graphs where the shortest distance node is always chosen next a algorithm is similar to dijkstras algorithm with the added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a graph database system that supports both transactional and analytical processing of graphbased data and it is a relatively new class of nosql dbs neo4j is considered schema optional one can be imposed and supports various types of indexing neo4j is acid compliant and supports distributed computing"
                ]
            }
        },
        {
            "title": "ICS 46 Spring 2022, Notes and Examples- AVL Trees",
            "chunked_content": {
                "200_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet",
                    "that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up",
                    "to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "200_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too",
                    "anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be",
                    "h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least",
                    "not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions",
                    "empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2",
                    "8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "200_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a",
                    "in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree",
                    "very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the",
                    "anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to",
                    "start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply",
                    "add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be",
                    "too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and",
                    "except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness",
                    "we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good",
                    "this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet",
                    "not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search",
                    "work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and",
                    "point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique",
                    "runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up",
                    "empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free",
                    "despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2",
                    "an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result",
                    "n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness",
                    "to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals",
                    "despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm",
                    "transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they",
                    "larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree",
                    "the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "500_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you",
                    "tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9",
                    "a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even",
                    "well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like",
                    "wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now",
                    "trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still",
                    "of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_0": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root",
                    "node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links",
                    "are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh",
                    "h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_50": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes",
                    "a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees",
                    "node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats",
                    "while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ],
                "1000_words_overlap_100": [
                    "ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes smaller keys have to be in left subtrees and larger keys in right subtrees but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys when all you care about is that theyre unique is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack one for every recursive call there are ways to mitigate this for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its",
                    "10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h 1 and the right subtree is a complete binary tree of height h 1 the left subtree is a complete binary tree of height h 1 and the right subtree is a perfect binary tree of height h 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left which is complete by our definition and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered wellenough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that",
                    "and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance heights of subtrees can be slightly different but no more than that in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t1 t2 and t3 written in them are arbitrary subtrees which may be empty or may contain any number of nodes but which are themselves binary search trees its important to remember that both of these trees before and after are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t1 t2 and t3 maintain the appropriate positions relative to the keys a and b all keys in t1 are smaller than a all keys in t2 are larger than a and smaller than b all keys in t3 are larger than b performing this rotation would be a simple matter of adjusting a few pointers notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t2 bs left child would now be the root of t2 instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its",
                    "when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree which is empty has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations ll rr lr or rl will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation the first two links leading from 40 down toward 35 are a left and a right rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t1 the empty left subtree of the node containing 35 is t2 the empty right subtree of the node containing 35 is t3 the empty right subtree of the node containing 40 is t4 after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t1 t2 t3 and t4 were all empty so they are still empty note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree potentially all the way down to a leaf position then all the way back up if the length of the longest path thats what the height of a tree is is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is",
                    "to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h 1 with the minimum number of nodes the other of which is an avl tree with height h 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights thats fairly selfexplanatory which means that we can be sure that 1 mh 1 mh 2 given that we can conclude the following mh 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh 2mh 2 22mh 4 4mh 4 42mh 6 8mh 6 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 2h2mh h 2h2m0 mh 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh 2h2 log2mh h2 2 log2mh h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log2n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log2n its something more akin to about 144 log2n even for avl trees with the minimum number of nodes though the proof of that is more involved and doesnt change the asymptotic result"
                ]
            }
        },
        {
            "title": "Lecture 2 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst",
                    "worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "200_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but",
                    "for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison",
                    "slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on",
                    "worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent",
                    "linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "500_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow",
                    "disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_0": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_50": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ],
                "1000_words_overlap_100": [
                    "searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for you get to the last element and havent found it record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute list of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single chunk of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses arrays are faster for random access but slow for inserting anywhere but the end linked lists are faster for inserting anywhere in the list but slower for random access observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binary_searcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log2 n comparisons therefore in the worst case binary search is olog2n time complexity back to database searching assume data is stored on disk by column ids value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column cant store data on disk sorted by both id and specialval at the same time data would have to be duplicated space inefficient we need an external data structure to support faster searching by specialval than a linear scan 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
                ]
            }
        },
        {
            "title": "Lecture 3 Notes (B-Trees) - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "structures and algorithms chapter 11"
                ],
                "200_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of",
                    "side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "200_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual",
                    "a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either",
                    "elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers",
                    "side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the",
                    "a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant",
                    "root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data",
                    "3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11",
                    "structures and algorithms chapter 11"
                ],
                "500_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "500_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "500_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from",
                    "to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11",
                    "structures and algorithms chapter 11"
                ],
                "1000_words_overlap_0": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "1000_words_overlap_50": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ],
                "1000_words_overlap_100": [
                    "btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11"
                ]
            }
        },
        {
            "title": "Lecture 4 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "200_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default",
                    "changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "200_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data",
                    "unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has",
                    "no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check",
                    "changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are",
                    "schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed",
                    "modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no",
                    "databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "500_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes",
                    "if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency",
                    "guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_0": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_50": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ],
                "1000_words_overlap_100": [
                    "benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints isolation two transactions t1 and t2 are being executed at the same time but cannot affect each other if both t1 and t2 are reading the data no problem if t1 is reading the same data that t2 may be writing can result in dirty read nonrepeatable read phantom reads durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved isolation dirty read dirty read a transaction t1 is able to read a row that has been modified by another transaction t2 that hasnt yet executed a commit isolation nonrepeatable read nonrepeatable read two queries in a single transaction t1 execute a select but get different values because another transaction t2 has changed data and committed isolation phantom reads phantom reads when a transaction t1 is running and another transaction t2 adds or deletes rows from the set t1 is using example transaction transfer delimiter create procedure transfer in sender_id int in receiver_id int in amount decimal102 begin declare rollback_message varchar255 default transaction rolled back insufficient funds declare commit_message varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where account_id sender_id attempt to credit money to account 2 update accounts set balance balance amount where account_id receiver_id check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where account_id sender_id 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set message_text rollback_message else log the transactions if there are sufficient funds insert into transactions account_id amount transaction_type values sender_id amount withdrawal insert into transactions account_id amount transaction_type values receiver_id amount deposit commit the transaction commit select commit_message as result end if end delimiter however relational databases may not be the solution to all problems sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions a single main node can go down two paths replication or sharding replication replication copies data to multiple servers sharding sharding divides data across multiple servers distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure",
                    "partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure"
                ]
            }
        },
        {
            "title": "Lecture 5 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "200_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from",
                    "maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry",
                    "1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "200_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme",
                    "there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can",
                    "pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but",
                    "maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or",
                    "very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of",
                    "testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does",
                    "a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model",
                    "is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300",
                    "friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network",
                    "internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "500_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are",
                    "response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set",
                    "not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops",
                    "logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_0": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_50": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ],
                "1000_words_overlap_100": [
                    "distributed dbs and acid pessimistic concurrency acid transactions focuses on data safety considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy borrowing a book from a library if you have it no one else can optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data acid alternative for distribution systems base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated keyvalue databases keyvalue stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins they slow things down scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing storing session information everything about the current session can be stored via a single put or post and retrieved with a single get very fast user profiles preferences user info could be obtained with a single get operation language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series it is considered an inmemory database system but supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didnt set a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments",
                    "a password connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others other list ops llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark sismember ds4300 nick scard ds4300 sadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 mark srandmember ds4300"
                ]
            }
        },
        {
            "title": "Lecture 6 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "200_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "200_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful",
                    "4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "500_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_0": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_50": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ],
                "1000_words_overlap_100": [
                    "redis in docker setup prerequisities you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure and run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis click ok if connection test was successful"
                ]
            }
        },
        {
            "title": "Lecture 7 Notes - Mihalis Koutouvos",
            "chunked_content": {
                "200_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "200_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4",
                    "i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "200_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list",
                    "to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for",
                    "commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4",
                    "i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "500_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_0": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_50": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ],
                "1000_words_overlap_100": [
                    "redis and python redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decode_responses data comes back from server as bytes setting this true converter them decodes to strings import redis redis_client redisredishostlocalhost port6379 db2 decode_responsestrue redis command list full list here use filter to get to command for the particular data structure youre targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rsetclickcountabc 0 val rgetclickcountabc rincrclickcountabc ret_val rgetclickcountabc printfclick count ret_val string commands 2 r represents the redis client object redis_clientmsetkey1 val1 key2 val2 key3 val3 printredis_clientmgetkey1 key2 key3 returns as list val1 val2 val3 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key names values mark sam nick redis_clientrpushnames mark sam nick prints mark sam nick printredis_clientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redis_clienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredis_clienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server less network overhead r redisredisdecode_responsestrue pipe rpipeline for i in range5 pipesetfseati fi set_5_result pipeexecute printset_5_result true true true true true pipe rpipeline chain pipeline commands together get_3_result pipegetseat0getseat3getseat4execute printget_3_result 0 3 4"
                ]
            }
        }
    ]
}